# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oPtZiUixZ7YasX9vf3fBGbienZecApBZ
"""

!wget -nc https://lazyprogrammer.me/course_files/AirlineTweets.csv

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

np.random.seed(1)

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score
from sklearn.model_selection import train_test_split

df = pd.read_csv('AirlineTweets.csv')

df.head()

df = df.drop(['tweet_id', 'airline_sentiment_confidence', 'negativereason', 'negativereason_confidence', 'airline', 'airline_sentiment_gold', 'name', 'negativereason_gold', 'retweet_count', 'tweet_coord', 'tweet_created', 'tweet_location', 'user_timezone'], axis=1)

df.head()

df.columns = ['labels', 'text']

df.head()

df['labels'].hist()

df["target"] = df['labels'].map({'neutral':2, 'positive':1, 'negative':0})
df.head()

# Y = df['target'].to_numpy()

# print(Y)

# df_train, df_test, Y_train, Y_test = train_test_split(df['text'], Y, test_size=0.2)

# featurizer = CountVectorizer(decode_error='ignore')
# Xtrain = featurizer.fit_transform(df_train)
# Xtest = featurizer.transform(df_test)

# Xtrain

# model = LogisticRegression(solver='liblinear')
# model.fit(Xtrain, Y_train)

# print("train score", model.score(Xtrain, Y_train))
# print("test score", model.score(Xtest, Y_test))

df_train, df_test = train_test_split(df)

df_train.head()

vectorizer = TfidfVectorizer(max_features=2000)

X_train = vectorizer.fit_transform(df_train['text'])
X_test = vectorizer.transform(df_test['text'])

X_train

Y_train = df_train['target']
Y_test = df_test['target']

model = LogisticRegression(max_iter=500)
model.fit(X_train, Y_train)

print("train score", model.score(X_train, Y_train))
print("test score", model.score(X_test, Y_test))

Pr_train = model.predict_proba(X_train)
Pr_test = model.predict_proba(X_test)

print("Train AUC:", roc_auc_score(Y_train, Pr_train, multi_class='ovo'))
print("Test AUC:", roc_auc_score(Y_test, Pr_test, multi_class='ovo'))

P_train = model.predict(X_train)
P_test = model.predict(X_test)

print("Train Accuracy:", accuracy_score(Y_train, P_train))
print("Test Accuracy:", accuracy_score(Y_test, P_test))

cm = confusion_matrix(Y_train, P_train, normalize='true')
cm

def plot_cm(cm):
  classes = ['negative', 'positive', 'neutral']
  df_cm = pd.DataFrame(cm, index=classes, columns=classes)
  ax = sns.heatmap(df_cm, annot=True, fmt='g')
  ax.set_title('Confusion Matrix')
  ax.set_xlabel('Predicted')
  ax.set_ylabel('Target')

plot_cm(cm)

cm_test = confusion_matrix(Y_test, P_test, normalize='true')
cm_test

plot_cm(cm_test)

"""## Binary Labels & Model Interpretation"""

target_map = {'neutral':2, 'positive':1, 'negative':0}

binary_target_list = [target_map['positive'], target_map['negative']]
df_b_train = df_train[df_train['target'].isin(binary_target_list)]
df_b_test = df_test[df_test['target'].isin(binary_target_list)]

df_b_train.head()

X_train = vectorizer.fit_transform(df_b_train['text'])
X_test = vectorizer.transform(df_b_test['text'])

Y_train = df_b_train['target']
Y_test = df_b_test['target']

model = LogisticRegression(max_iter=500)
model.fit(X_train, Y_train)

print("train score", model.score(X_train, Y_train))
print("test score", model.score(X_test, Y_test))

Pr_train = model.predict_proba(X_train)[:, 1]
Pr_test = model.predict_proba(X_test)[:, 1]

print("Train AUC:", roc_auc_score(Y_train, Pr_train))
print("Test AUC:", roc_auc_score(Y_test, Pr_test))

model.coef_

plt.hist(model.coef_[0], bins=10)

word_index_map = vectorizer.vocabulary_
word_index_map

# let's look at the weights for each
# try it with different threshold values!

threshold = 2

print("Most Positive words")
for word, index in word_index_map.items():
  weight = model.coef_[0][index]
  if weight > threshold:
    print(word, weight)

print("Most Negative words")

for word, index in word_index_map.items():
  weight = model.coef_[0][index]
  if weight < -threshold:
    print(word, weight)

# Exercise:print the most-wrong tweets for each both classes
# i.e find a negative review where p(y = 1 | x) is closest to 1
#     find a positive review where p(y = 1 | x) is closest to 0

# Excercise: set class_weight="balanced"