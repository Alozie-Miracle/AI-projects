# -*- coding: utf-8 -*-
"""Text Summarization using TextRank.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NoIPaD3R2zf0HX6ShWXRoS6VpG_njjqO
"""

import pandas as pd
import numpy as np
import textwrap
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

nltk.download('stopwords')
nltk.download('punkt_tab')
nltk.download('wordnet')

!wget -nc https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv

df = pd.read_csv('bbc_text_cls.csv')

df.head()

df['labels'].hist()

doc = df[df.labels == 'business']['text'].sample(random_state=42)
doc.iloc[0]

def wrap(x):
  return textwrap.fill(x, replace_whitespace=False, fix_sentence_endings=True)

print(wrap(doc.iloc[0]))

sentences = nltk.sent_tokenize(doc.iloc[0].split("\n", 1)[1])
sentences

featurizer = TfidfVectorizer(stop_words=stopwords.words('english'), norm='l1')

x = featurizer.fit_transform(sentences)
x

# compute cosine similarity
S = cosine_similarity(x)

S.shape

len(sentences)

# normalize similarity matrix
S /= S.sum(axis=1, keepdims=True)

S[0].sum()

# uniform transition matrix
U = np.ones_like(S) / len(S)

U[0].sum()

# smoothed similarity matrix
factor = 0.15
S = (1 - factor) * S + factor * U

S[0].sum()

# find the limiting / stationary dstribution
eigenvalues, eigenvectors = np.linalg.eig(S.T)

eigenvalues

eigenvectors[:,0]

eigenvectors[:,0].dot(S)

eigenvectors[:,0] / eigenvectors[:,0].sum()

limiting_dist = np.ones(len(S)) / len(S)
threshold = 1e-8
delta = float('inf')
iters = 0
while delta > threshold:
  iters += 1
  # update limiting distribution
  limiting_dist_new = limiting_dist.dot(S)

  # compute change in limiting distribution
  delta = np.abs(limiting_dist_new - limiting_dist).sum()

  # update limiting distribution
  limiting_dist = limiting_dist_new

print(iters)

limiting_dist

limiting_dist.sum()

np.abs(eigenvectors[:,0] / eigenvectors[:,0].sum() - limiting_dist).sum()

scores = limiting_dist

sort_idx = np.argsort(-scores)

# Many options for how to choose which sentences to include:

# 1) top N sentences
# 2) top N words
# 3) top X% sentences or top X% words
# 4) sentences with scores > average score
# 5) sentencesnwith scores > factor * average score

# You also don't have to sort. May make more sense in order.
print("Generate Summary: ")
for i in sort_idx[:5]:
  print(wrap("%.2f: %s" % (scores[i] * 100, sentences[i])))

doc.iloc[0].split('\n', 1)[0]

def summarize(text, factor=0.15):
  # extract sentences
  sents = nltk.sent_tokenize(text)

  # perform tf-idf
  featurizer = TfidfVectorizer(stop_words=stopwords.words('english'), norm='l1')
  x = featurizer.fit_transform(sents)

  # compute similarity matrix
  S = cosine_similarity(x)

  # normalize similarity matrix
  S /= S.sum(axis=1, keepdims=True)

  # uniform transition matrix
  U = np.ones_like(S) / len(S)

  # smoothed similarity matrix
  S = (1 - factor) * S + factor * U

  # eigen values and eigen vectors
  eigenvalues, eigenvectors = np.linalg.eig(S.T)

  # compute score
  scores = eigenvectors[:, 0] / np.sum(eigenvectors[:, 0])

  # sort scores
  sort_idx = np.argsort(-scores)

  # print summary
  for i in sort_idx[:5]:
    print(wrap("%.2f: %s" % (scores[i] * 100, sents[i])))

doc = df[df.labels == 'tech']['text'].sample(random_state=123)
summarize(doc.iloc[0].split('\n', 1)[1])

doc.iloc[0].split('\n', 1)[0]

articles = df[df.labels == 'business']['text'].sample(random_state=42)
summarize(articles.iloc[0].split('\n', 1)[1])

